---
title: "DSI_Lab_6"
author: "Dingxin Lu"
date: "12/6/2021"
output: 
  html_document:
   df_print: paged
   toc: true 
   toc_depth: 2  
   number_sections: false
   toc_float:
     collapsed: true
     smooth_scroll: true
---

[My Github link] https://github.com/sydneydlu98/DSI_Lab_6-Principal-component-analysis

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, fig.align = "center")
```

# Simulate Data

We will begin by simulating some data on which weâ€™ll perform principal component analysis (PCA) and clustering. Use the following code to simulate data for this exercise. We will be simulating data from two groups.

```{r}
## load in required libraries 
library(hbim)
library(mvtnorm)
library(dplyr)
library(ggplot2)

## set a seed for reproducibility
set.seed(12345)

## create an exchangeable variance covariance for the data
sigma <- make.v(n = 100, r = .6, sig2 = 1)

## create centers for the two clusters. 
center_1 <- rep(1, 100)
center_2 <- rep(3, 100)

## simulate data for two groups from a multivariate normal distribution 
data <- rbind(rmvnorm(50, mean = center_1, sigma = sigma),
             rmvnorm(50, mean = center_2, sigma = sigma))

## add a group label to the data 
data <- data.frame(group = c(rep(1, 50), rep(2, 50)),
                  data) 
```

# Visualize the Data

Next we will visulaize the data.

* Look at the correlation of the data using corrplot::corrplot. Comment on what you observe.

```{r}
## plot the correlation of the data
corrplot::corrplot(
  cor(data, 
      method = 'spearman'),
  method = 'shade',
  tl.cex = 0.6,
  order = "hclust",
  tl.col = 'black',
  tl.srt = 90
)
```

**From the correlation plot of the data, we can see that all these 100 variables (X1 to X100) have very high positive correlations with each other, meaning they all have correlation coefficients of 1, 0.9, 0.8 and 0.7 with each other. It tells us that these 100 variables are highly correlated with each other.**

* Create density plots colored by group membership of the first three variables in the data set. Comment on what you observe.
```{r}
## we filter the data to make the correlation plot
data_3 <- data %>%
  select(group:X3)

## create density plots colored by group membership 
ggplot(data_3,
       aes(x = X1,
           group = group,
           fill = as.factor(group))) +
  geom_density(aes(alpha = 0.5)) +
  ggtitle("Density plot of X1 colored by group membership") +
  labs(fill = "group") +
  guides(alpha = FALSE) +
  theme(plot.title = element_text(hjust = 0.5,
                                  size = 16))

ggplot(data_3,
       aes(x = X2,
           group = group,
           fill = as.factor(group))) +
  geom_density(aes(alpha = 0.5)) +
  ggtitle("Density plot of X2 colored by group membership") +
  labs(fill = "group") +
  guides(alpha = FALSE) +
  theme(plot.title = element_text(hjust = 0.5,
                                  size = 16))

ggplot(data_3,
       aes(x = X3,
           group = group,
           fill = as.factor(group))) +
  geom_density(aes(alpha = 0.5)) +
  labs(fill = "group") +
  guides(alpha = FALSE) +
  ggtitle("Density plot of X3 colored by group membership") +
  theme(plot.title = element_text(hjust = 0.5,
                                  size = 16))
```

**These 3 plots all look roughly similar to each other, there are small discrepancies among them but overall they all have similar distributions and have similar areas that overlap with each group. It means values between 2 groups in X1, X2 and X3 are very similar to each other.**

**However, we can notice that for all plots of X1, X2 and X3, distributions for group 1 are all slightly right skewed and distributions for group 2 are all left skewed.**

# Perform PCA

* Perform PCA on the data.

```{r}
## data cleaning for PCA
new_data <- data %>%
  select(-group)

## Perform PCA on the data
pca <- prcomp(new_data, 
              center = TRUE, 
              scale. = TRUE)
```

* Make a plot of the cumulative variance explained by the PCs.

```{r}
## calculate variance explained by the PCs
eigs <- pca$sdev ^ 2
var.explained <- eigs / sum(eigs)

## create a plot of the cumulative variance explained by the PCs
plot(
  cumsum(var.explained),
  main = 'Plot of cummulative variance explained',
  ylab = 'Cummulative variance explained',
  xlab = 'Number of PCs'
)
```

* Make bivariate plots of all combinations of the scores on the first, second, and third PC (PC1 vs. PC2, PC1 vs. PC3, PC2 vs. PC3) colored by group membership.

```{r}
## make the data frame for the scores on the PCs
pca_scores <- as.data.frame(pca$x)

## add 2 columns to make the plots
pca_scores <- pca_scores %>%
  mutate(group = data$group)

## Make bivariate plots of all combinations of the scores on the first, second, and third PC
## PC1 vs. PC2
ggplot(pca_scores,
       aes(x = PC1,
           y = PC2,
           col = as.factor(group))) +
  geom_point() +
  labs(col = "group") +
  ggtitle("PC1 versus PC2") +
  theme(plot.title = element_text(hjust = 0.5,
                                  size = 16))

## PC1 vs. PC3
ggplot(pca_scores,
       aes(x = PC1,
           y = PC3,
           col = as.factor(group))) +
  geom_point() +
  labs(col = "group") +
  ggtitle("PC1 versus PC3") +
  theme(plot.title = element_text(hjust = 0.5,
                                  size = 16))

## PC2 vs. PC3
ggplot(pca_scores,
       aes(x = PC2,
           y = PC3,
           col = as.factor(group))) +
  geom_point() +
  labs(col = "group") +
  ggtitle("PC2 versus PC3") +
  theme(plot.title = element_text(hjust = 0.5,
                                  size = 16))
```

# Cluster

* Cluster the original data into 2 clusters using any method that we have learned in this class. Create a contingency matrix with the true cluster labels.

```{r}
## set seed
set.seed(12345)

## scale the filtered data
data_scaled <- scale(new_data, 
                     center = TRUE, 
                     scale = TRUE)

## select 2 clusters
k.means <- kmeans(data_scaled,
                  centers = 2,
                  nstart = 100)

## contingency table of number of observations in each cluster and each group
table(cluster = k.means$cluster, 
      group = data$group)
```

* Rather than performing clustering on the entire data matrix, we can simply perform clustering on the first few principal component score vectors. Sometimes performing clustering on the first few principal component score vectors can give better results than performing clustering on the full data with the results. Repeat the clustering with the first 10 principal component scores and create a contingency matrix.

```{r}
## set seed
set.seed(12345)

## select the first 10 principal component scores
pca_10 <- pca_scores %>%
  select(PC1:PC10)

## scale this new data set
pca_10_scaled <- scale(pca_10, 
                       center = TRUE, 
                       scale = TRUE)

## select 2 clusters
k.means_10 <- kmeans(pca_10_scaled,
                     centers = 2,
                     nstart = 100)

## contigency table of number of observations in each cluster and each group
table(cluster = k.means_10$cluster, 
      group = data$group)
```

* Comment on what you observe.

**Clearly after clustering with the first 10 principal component scores, the contingency matrix seems more "balanced" as number of each cluster in each group are roughly around 25. It means, roughly around 25 observations are correctly classified in each group. It actually shows a worse result as less observations are being correctly classified. **

**In conclusion, sometimes performing clustering on the first few principal component score vectors can give better results than performing clustering on the full data with the results. However, in our case it is not true, it is actually better to perform clustering on the full data.**